{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical 08: Introduction to Reinforcement Learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\";> <b>Lets first make sure that all the required dependencies are installed</b></p> \n",
    "\n",
    "### With pip (in your local machine)\n",
    "1. Open Anaconda prompt\n",
    "2. Type ``pip install gym``\n",
    "3. If pip is missing, type ``conda install pip``\n",
    "\n",
    "### In AWS\n",
    "1. Add and execute the following line at the beginning of your notebook\n",
    "```python \n",
    "import sys\n",
    "!{sys.executable} -m pip install gym```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us first import all the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('Support'))\n",
    "\n",
    "from gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "from gym_simple_gridworlds.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define an MDP (Grid World)\n",
    "\n",
    "Recall the grid in which our robot lives\n",
    "\n",
    "<img src=\"Support/images/GridWorldExample.png\" width=\"400\" height=\"400\" align=\"center\">\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (1,0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (1,0)\n",
    "    - Down: (-1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "\n",
    "We have defined the class ``GridEnv`` to represent our Grid World MDP. **Take a look at the attributes of this class by placing the cursor somewhere on the class' name and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up.**\n",
    "\n",
    "Note: In this implementation, for simplicity, the reward and state transition functions are indexed by the cell and action indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the current state transition function. By default, our state transition function is deterministic. For example, at the state (1, 2), i.e., 5th cell, if the agent moves up, it will transition with 100% probability to the state (2, 2), i.e, the 9th cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_world.state_transitions[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic vs Stochastic State Transitions\n",
    "\n",
    "Let's see how the path of an agent moving on our grid world changes with the definition of the state transition function.\n",
    "\n",
    "We start defining a deterministic policy matrix that indicates which action is executed by the agent at a given state. No actions are available at terminal states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_matrix = np.array([[3,      3,  3,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      1,  1,   1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this policy, in a grid world with a deterministic transition function, the agent can reach the goal state (green rectangle) with the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_state = grid_world.cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "while not done:\n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(policy_matrix[cur_state[0], cur_state[1]]))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a stochastic transition matrix by setting the noise attribute to a non-zero value. We will use 0.5.\n",
    "\n",
    "With this new transition function, at the state (1,2), i.e., 5th cell, if the agent chooses to move up, it will end up at:\n",
    "- state (2, 2), i.e, the 9th cell with $50\\%$ probability,\n",
    "- state (1, 2), i.e, the 5th cell with $25\\%$ probability, or\n",
    "- state (1, 3), i.e, the 6th cell with $25\\%$ probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new grid world environment and set noise=0.5\n",
    "grid_world = GridEnv(noise=0.5)\n",
    "\n",
    "# Let's see how the transition matrix changes\n",
    "print(grid_world.state_transitions[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flux Question**: What happens to the cumulative reward when we define a stochastic state transition function (``noise=0.5``)? Execute the code below and see how the value displayed at the top-left corner changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(noise=0.5)\n",
    "cur_state = grid_world.cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "while not done:\n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(policy_matrix[cur_state[0], cur_state[1]]))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterative Policy Evaluation\n",
    "\n",
    "Recall the definition of the iterative policy evaluation algorithm\n",
    "\n",
    "<img src=\"Support/images/IterativePolicyEvaluation.png\" width=\"600\" height=\"600\" align=\"center\">\n",
    "\n",
    "Let's now compute the value function of the following policy $\\pi$\n",
    "\n",
    "<img src=\"Support/images/PolicyEvaluation.png\" width=\"300\" height=\"300\" align=\"center\">\n",
    "\n",
    "We consider a grid world environment with the following attributes:\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=0.0``)\n",
    "\n",
    "We have defined the helper function ``encode_policy()`` to encode the policy $\\pi$ shown in the image above. The return variable ``policy_pi`` is a dictionary of dictionaries, where each element corresponds to the probability of selecting an action $a$ at a given state $s$\n",
    "\n",
    "Keep in mind that each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0.0)\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# This is the probability of selection the action (Up) at cell 0 (state (0,0) in the grid)\n",
    "print(policy_pi[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the policy $\\pi$, let's know compute its state-value function using iterative policy evaluation.\n",
    "\n",
    "**TODO**: \n",
    "- Complete the computation of the discounted value of successor states $\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v(s')$\n",
    "- Keep in mind that the attribute ``grid_env.state_transitions`` corresponds to the state transition function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(grid_env, policy, plot=False, threshold=0.00001):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function computes the value function for a policy pi in a given environment grid_env.\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param policy (dict - stochastic form): Policy being evaluated\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "        \n",
    "    # Obtain list of all states in environment\n",
    "    v = {s: 0.0 for s in grid_env.get_states()}\n",
    "    theta = threshold\n",
    "    delta = 1000\n",
    "\n",
    "    while delta > theta:\n",
    "        delta = 0.0\n",
    "        # For all states\n",
    "        for s in v.keys():\n",
    "\n",
    "            old_v = v[s]\n",
    "            new_v = 0\n",
    "\n",
    "            # For all actions\n",
    "            for a, probability_a in policy[s].items():\n",
    "                discounted_value = 0\n",
    "\n",
    "                # For all states that are reachable from s with action a\n",
    "                for s_next in grid_env.get_states():\n",
    "                    # TODO: Complete the computation of the discounted value of successor states\n",
    "                    pass\n",
    "                    \n",
    "                # Compute new value for state s\n",
    "                new_v += probability_a*(grid_env.rewards[s, a] + grid_env.gamma*discounted_value)\n",
    "\n",
    "            v[s] = new_v\n",
    "            delta = max(delta, np.abs(old_v - new_v))\n",
    "\n",
    "    if plot:\n",
    "        plot_value_function(grid_env, v)\n",
    "        \n",
    "    return v\n",
    "        \n",
    "        \n",
    "# Call the policy evalution function\n",
    "v = policy_evaluation(grid_world, policy_pi, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flux Question**: What is the value of state (2,0)? Keep in mind that coordinates are given in (row, col) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of the policy iteration algorithm\n",
    "\n",
    "<img src=\"Support/images/PolicyIteration.png\" width=\"600\" height=\"600\" align=\"center\">\n",
    "\n",
    "Starting with a random policy, let's find the optimal policy for a grid world environment with attributes:\n",
    "\n",
    "We consider a grid world environment with the following attributes:\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=0.0``)\n",
    "\n",
    "We will first define the helper methods:\n",
    "- ``one_step_lookahead(grid_env, state, value_function)``, this method computes the action-value function for a state $s$ given the state-value function $v$. This corresponds to $\\mathcal{R}(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v_\\pi(s')\\, \\forall \\, a \\in \\mathcal{A}$\n",
    "- ``update_policy(grid_world, policy, value_function)``, this method updates the current policy $\\pi$ given the state-value function $v$ by taking the action $a$ with the highest action-value. action-values are obtained using the function\n",
    "- ``define_random_policy(grid_env)`` in script ``helper.py``, this method generates a random policy for environment ``grid_env``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(grid_env, state, value_function):\n",
    "    \"\"\"\n",
    "     Compute the action-value function for a state $s$ given the state-value function $v$.\n",
    "     \n",
    "     :param grid_env (GridEnv): MDP environment\n",
    "     :param state (int): state for which we are looking one action ahead\n",
    "     :param value_function (dict): state-value function associated to a given policy py\n",
    "     \n",
    "     :return: (np.array) Action-value functions of actions available at state s\n",
    "    \"\"\"\n",
    "    action_values = []\n",
    "    \n",
    "    for action in grid_env.get_actions():\n",
    "        discounted_value = 0\n",
    "        for s_next in grid_env.get_states():\n",
    "             discounted_value += grid_env.state_transitions[state, action, s_next] * value_function[s_next]\n",
    "        \n",
    "        q_a = grid_env.rewards[state, action] + grid_env.gamma * discounted_value\n",
    "        action_values.append(q_a)\n",
    "    \n",
    "    return np.array(action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(grid_env, cur_policy, value_function):\n",
    "    \"\"\"\n",
    "     Update a given policy based on a given value_function\n",
    "     \n",
    "     :param grid_env (GridEnv): MDP environment\n",
    "     :param cur_policy (matrix form): Policy to update\n",
    "     :param value_function: state-value function associated to a policy cur_policy\n",
    "     \n",
    "     :return: (dict) Updated policy\n",
    "    \"\"\"\n",
    "    \n",
    "    states = grid_env.get_states(exclude_terminal=True)\n",
    "    \n",
    "    for s in states:\n",
    "        action_values = one_step_lookahead(grid_env, s, value_function)\n",
    "        \n",
    "        # Find (row, col) coordinates of cell with index s\n",
    "        row, col = np.argwhere(grid_env.grid == s)[0]\n",
    "        \n",
    "        cur_policy[row, col] = np.argmax(action_values)\n",
    "        \n",
    "    return cur_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the policy iteration core algorithm.\n",
    "\n",
    "**TODO**: Complete the main steps of the policy iteration algoritm.\n",
    "- Use ``policy_evaluation(.)`` to compute the state-value function of a given policy\n",
    "- Use ``update_policy(.)`` to obtain an updated policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(grid_env, policy, plot=False):\n",
    "    \"\"\"\n",
    "    This function iteratively updates a given policy pi for a given environment grid_env until convergence to optimal policy\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param policy (matrix from): Deteministic policy being updated\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "    prev_policy = np.zeros(policy.shape)\n",
    "    \n",
    "    while not np.all(np.equal(prev_policy, policy)):\n",
    "        \n",
    "        # Encode policy. This policy representation is needed for policy evaluation\n",
    "        encoded_policy = encode_policy(grid_env, policy)\n",
    "        # Set prev_policy to current policy\n",
    "        prev_policy = policy.copy()\n",
    "        \n",
    "        #TODO: Complete the remaining steps\n",
    "        # 1. Evaluate the given policy (policy evaluation expects the enconded policy)\n",
    "\n",
    "        # 2. Update policy using helper function update_policy\n",
    "\n",
    "        \n",
    "    if plot:\n",
    "        plot_policy(grid_env, policy)\n",
    "    \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid world mdp\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0.0)\n",
    "\n",
    "# Generate an initial random policy\n",
    "initial_policy = define_random_policy(grid_world)\n",
    "\n",
    "# Compute optimal policy using policy iteration\n",
    "optimal_policy = policy_iteration(grid_world, initial_policy, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flux Question**: What is the optimal sequence of actions to go from state (0,1) to state (2,3)? Keep in mind that coordinates are given in (row, col) format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Exercise (6 pts) - Implement Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are tasked with implementing the value iteration algorithm shown below:\n",
    "    \n",
    "<img src=\"Support/images/ValueIteration.png\" width=\"500\" height=\"500\" align=\"center\">\n",
    "\n",
    "Please keep in mind:\n",
    "- **TODO 1**: Use the helper method ``one_step_lookahead(.)`` to update of your current $v(s)$ according to:\n",
    "$v(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\Bigl(\\mathcal{R}(s,a) + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{T}(s,a,s')v(s')\\Bigr)$\n",
    "- **TODo 2**: Use the helper method ``update_policy(.)`` to obtain the deterministic optimal policy $\\pi^*$ associated to your $v^*(s)$ estimate\n",
    "\n",
    "Expected return:\n",
    "- Your implementation should return the state-value function (dictionary) and the optimal policy (matrix form)\n",
    "\n",
    "For grading:\n",
    "- You will be graded based on the output of the ``value_iteration(.)`` method. Please make sure that your solution returns the expected variables with the correct type (no rounding is needed)\n",
    "- Remove all print statements before submitting your solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid_env, threshold=0.00001, plot=False):\n",
    "    \"\"\"\n",
    "    This function iteratively computes optimal state-value function for a given environment grid_env. \n",
    "    It returns the optimal state-value function and its associated optimal policy\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param threshoold (float): Convergence threshold\n",
    "    :param plot (bool): Bool argument indicating if value function and policy should be displayed \n",
    "    :return: (tuple) Optimal state-value funciton (dict) and deterministic optimal policy (matrix)\n",
    "    \"\"\"\n",
    "    \n",
    "    #1. Get list of states in environment\n",
    "    states = grid_env.get_states()\n",
    "    \n",
    "    #2. Initialize v function\n",
    "    v = {s: 0.0 for s in grid_env.get_states()}\n",
    "    \n",
    "    #3. Set convergence threshold and error variable\n",
    "    theta = threshold\n",
    "    delta = 1000\n",
    "    \n",
    "    #4. Update v(s) until convergence\n",
    "    while delta > theta:\n",
    "        \n",
    "        ## TODO 1: Compute computation of state-value function.\n",
    "        pass\n",
    "            \n",
    "    #5 Create instance of policy matrix\n",
    "    temp_policy = np.ones(grid_env.grid.shape) * -1\n",
    "    \n",
    "    #6. TODO 2: Update temp policy with deterministic optimal policy given v(s)\n",
    "    optimal_policy = 0\n",
    "    \n",
    "    \n",
    "    if plot:\n",
    "        plot_value_function(grid_env, v)\n",
    "        plot_policy(grid_env, optimal_policy)\n",
    "        \n",
    "    return v, optimal_policy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "\n",
    "Given an grid world environment with the following attributes:\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=0.0``)\n",
    "\n",
    "**Visually**: The plots of your state-value function and resulting optimal policy should be the same as the ones obtained in Section 2 and 3.\n",
    "\n",
    "**Numerically**: Compare your state-value function and optimal policy to the values  provided in the test file ``Support/data/ValueIteration_TestCase.pk``:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "with open('Support/data/ValueIteration_TestCase.pk', 'rb') as read_from:\n",
    "    test_values = pickle.load(read_from)\n",
    "    \n",
    "test_v = test_values['state_value_function']\n",
    "test_p = test_values['optimal_policy'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid world mdp\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0)\n",
    "\n",
    "optimal_state_function, optimal_policy = value_iteration(grid_world, plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentRobotics",
   "language": "python",
   "name": "intelligentrobotics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
