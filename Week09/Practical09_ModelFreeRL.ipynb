{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical 09: Part 1 - Model Free RL</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\";> <b>Let's first make sure that all the required dependencies are installed (this is only needed if gym was not installed successfully all time) </b></p> \n",
    "\n",
    "### With pip (in your local machine)\n",
    "1. Open Anaconda prompt\n",
    "2. Type ``pip install gym``\n",
    "3. If pip is missing, type ``conda install pip``\n",
    "\n",
    "### In AWS\n",
    "1. Add and execute the following line at the beginning of your notebook\n",
    "```python \n",
    "import sys\n",
    "!{sys.executable} -m pip install gym```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us first import all the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('Support'))\n",
    "\n",
    "from gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "from gym_simple_gridworlds.helper import *\n",
    "from gym_simple_gridworlds.dp_algorithms import *\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Monte Carlo Prediction Grid World Environment\n",
    "\n",
    "<img src=\"Support/images/GridWorldExample.png\" width=\"400\" height=\"400\" align=\"center\">\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (1,0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (1,0)\n",
    "    - Down: (-1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state\n",
    "- Discount factor $\\gamma = 0.9$ (class attribute ``gamma=0.9``)\n",
    "- Stochastic transition matrix (class attribute ``noise=0.2``)\n",
    "- Rewards are only obtained at terminal states (class attribute ``living_reward=0.0``)\n",
    "\n",
    "### Known Dynamics\n",
    "\n",
    "Recall the **optimal policy** we found using value-interation in the last lecture\n",
    "\n",
    "<img src=\"Support/images/PolicyEvaluation.png\" width=\"300\" height=\"300\" align=\"center\">\n",
    "\n",
    "since the dynamics of our grid world environment are known, we obtained the state-value function $v_\\pi(s)$ associated to this policy using ``policy_evalution(.)`` (This method has been added to the support files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0)\n",
    "\n",
    "# Get policy shown in image\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "plot_value_function(grid_world, v_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Visit Monte Carlo Policy Evaluation \n",
    "\n",
    "Let's first define ``generate_episode(.)``, a samples an episode i.e., a sequence of ($s, a, r, s'$) tuples from a given policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', ['state', 'action', 'reward', 'next_state'])\n",
    "\n",
    "def generate_episode(grid_env, policy):\n",
    "    \"\"\"\n",
    "    Generate an episode of experiences in environment under a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy used to sample actions\n",
    "    \n",
    "    :return List(Sample) Complete episode\n",
    "    \"\"\"\n",
    "    episode = []\n",
    "\n",
    "    # Reset the environment to a random initial state\n",
    "    state = grid_env.reset()\n",
    "\n",
    "    # Set flag to indicate whether episode has ended\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Get actions available at current state\n",
    "        all_actions = list(policy_pi[state].keys())\n",
    "        # Get action probabilities\n",
    "        all_probabilities = np.array(list(policy[state].values()))\n",
    "        # Sample an action from policy\n",
    "        action = np.random.choice(all_actions, 1, p=all_probabilities)[0]\n",
    "        \n",
    "        next_state, reward, done, info = grid_env.step(action)\n",
    "        episode.append(Sample(state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, under the assumption that $\\mathcal{T}(s,a,s')$ and $\\mathcal{R}(s,a)$ are unknown, let's use the algorithm shown below to get an estimate $\\hat{v}_\\pi(s)$ of the true state-value function $v_\\pi(s)$\n",
    "\n",
    "<img src=\"Support/images/MCPolicyEvaluation.png\" width=\"500\" height=\"500\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_first_visit_policy_evaluation(grid_env, policy, true_v, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Compute estimate of state-value function for a given policy\n",
    "    :param grid_env (GridEnv): Environment\n",
    "    :param policy (dict of probabilites): Policy to be evaluated\n",
    "    :param true_v (dict of floats): True state-value function. Used to compute prediciton error\n",
    "    :param n_episodes (int): Number of episodes to use for prediction\n",
    "    \n",
    "    :return List(float): Prediction error after each episode\n",
    "    :return dict(float): Predicted state-value function\n",
    "    \n",
    "    \"\"\"\n",
    "    all_states = grid_env.get_states()\n",
    "    \n",
    "    # Counter of visits for all states\n",
    "    state_visits = {s:0 for s in all_states}\n",
    "    # Cummulative return for each state\n",
    "    state_returns = {s:0 for s in all_states}\n",
    "    # Predicted state-value function\n",
    "    pred_v = {s:0 for s in all_states}\n",
    "    \n",
    "    # Variable used for plotting\n",
    "    list_errors = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        # Generate episode\n",
    "        episode = generate_episode(grid_env, policy)\n",
    "        # Create auxiliary variable to keep of first state visits\n",
    "        visited = {s: False for s in all_states}\n",
    "        # Return for current episode\n",
    "        g = 0\n",
    "        # Variable used to keep track of prediction error for this episode\n",
    "        error = 0\n",
    "        # Starting from last sampled observation in episode\n",
    "        for obs in episode[::-1]:\n",
    "            # Get visite state\n",
    "            s = obs.state\n",
    "            # Compute return for this state\n",
    "            g = g*grid_env.gamma + obs.reward    \n",
    "            # If this is the first time visiting this state\n",
    "            if not visited[s]:\n",
    "                # Increment the visit counter\n",
    "                state_visits[s] += 1\n",
    "                # Add return\n",
    "                state_returns[s] += g\n",
    "                # Compute mean return\n",
    "                pred_v[s] = state_returns[s]/float(state_visits[s])\n",
    "                # Compute error\n",
    "                error += np.abs(true_v[s] - pred_v[s])\n",
    "                # Set the state as visited\n",
    "                visited[s] = True\n",
    "        \n",
    "        list_errors.append(error)\n",
    "    \n",
    "    return list_errors, pred_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try the algorithm and compare its out to the true value-state function. \n",
    "\n",
    "**Interaction**:\n",
    "Run the algorithm multiple times and observe what happens when the number of episodes increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, predicted_v = monte_carlo_first_visit_policy_evaluation(grid_world, policy_pi, v_pi, n_episodes=100)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, :])\n",
    "\n",
    "#Plot true value function\n",
    "plot_value_function(grid_world, v_pi, f_ax1)\n",
    "f_ax1.set_title(\"True state-value function\")\n",
    "\n",
    "plot_value_function(grid_world, predicted_v, f_ax2)\n",
    "f_ax2.set_title(\"Predicted state-value function\")\n",
    "\n",
    "f_ax3.plot(errors)\n",
    "f_ax3.set_title(\"Predicted Error (sum of abs. differences)\")\n",
    "f_ax3.set_xlabel(\"Num. episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploration vs Exploitation\n",
    "\n",
    "Last time, our agent was able to explore the environment thanks to the noise in the transition function $\\mathcal{T}(s,a,s')$. However, in a model-free setting where our state-value and action-value estimates depend on the actions chosen by the agent, how can we guarantee that the all actions will continue to be selected?\n",
    "\n",
    "Recall the definition of an $\\epsilon$-greedy policy:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\pi(a|s) = \n",
    "    \\begin{cases}\n",
    "        1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}|},&  \\text{if } a^* = \\arg\\max_{a \\in \\mathcal{A}} q_\\pi(s,a)\\\\\n",
    "        \\frac{\\epsilon}{|\\mathcal{A}|}, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Let's see how the agent behaves when it follows an $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_egreedy_action(grid_env, state, q_value, epsilon):\n",
    "    \"\"\"\n",
    "    Select action to execute at a given state under an epsilon-greedy policy\n",
    "    :param grid_env (GridEnv): Grid world environment\n",
    "    :param state (int): Location in grid for which next action is going to be choosen\n",
    "    :param q_value (dict): Action-value function \n",
    "    :param epsilon (float): Randomness threshold used to choose action\n",
    "    \"\"\"\n",
    "    \n",
    "    rand_n = np.random.random()\n",
    "    if rand_n <= epsilon:\n",
    "        return grid_env.action_space.sample()\n",
    "    else:\n",
    "        actions = list(q_value[state].keys())\n",
    "        return actions[np.argmax(list(q_value[state].values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set noise to zero. Randomness in agent behaviour is only due to e-greedy policy\n",
    "grid_world = GridEnv(noise=0, living_reward=-0.04, gamma=0.99)\n",
    "\n",
    "# Get policy shown in section 1\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Compute value-function using dynamic programming\n",
    "v_pi = policy_evaluation(grid_world, policy_pi)\n",
    "\n",
    "# Use value-function to compute q-values\n",
    "q_pi = grid_world.get_q_values(v_pi)\n",
    "\n",
    "# Start episode\n",
    "cur_state = grid_world.idx_cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "v_epsilon = 0.8\n",
    "\n",
    "while not done:\n",
    "    action = get_egreedy_action(grid_world, cur_state, q_pi, v_epsilon)\n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(action))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interaction**:\n",
    "- Change $\\epsilon$ and observe how the agent behaviour changes.\n",
    "\n",
    "**FLUX Question**: What happens as $\\epsilon \\rightarrow 0$? Compare the behaviour of the agent to the case in which the optimal policy is used instead as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set noise to zero.\n",
    "grid_world = GridEnv(noise=0, living_reward=-0.04, gamma=0.99)\n",
    "\n",
    "# Get policy shown in section 1\n",
    "policy_pi = encode_policy(grid_world)\n",
    "\n",
    "# Start episode\n",
    "cur_state = grid_world.idx_cur_state\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "path_to_plot = []\n",
    "\n",
    "v_epsilon = 0.3\n",
    "\n",
    "while not done:\n",
    "    # Get actions available at current state\n",
    "    all_actions = list(policy_pi[cur_state].keys())\n",
    "    # Get action probabilities\n",
    "    all_probabilities = np.array(list(policy_pi[cur_state].values()))\n",
    "    # Pick an action\n",
    "    action = np.random.choice(all_actions, 1, p=all_probabilities)[0]\n",
    "    \n",
    "    cur_state, cur_reward, done, _ = grid_world.step(int(action))\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data(s_x + 0.5, s_y + 0.5)\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data(n_x + 0.5, n_y + 0.5)\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Q-Learning\n",
    "\n",
    "We have seen how to evaluate a policy without a model. Let's now find an *approximately* optimal policy using the off-policy control method Q-learning.\n",
    "\n",
    "To help during the learning, we have added a lambda function that iteratively decreases epsilon. Our agent will strongly explore the environment at first to then swicth into exploitation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_epsilon=0.001\n",
    "max_epsilon=1.0\n",
    "epsilon_decay = 80.0\n",
    "epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot([epsilon_by_episode(i) for i in range(500)])\n",
    "ax.set_xlabel(\"Num. episodes\")\n",
    "ax.set_ylabel(\"Epsilon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our implementation of the q-learning algorithm shown below\n",
    "\n",
    "<img src=\"Support/images/q-learning.png\" width=\"500\" height=\"500\" align=\"center\">\n",
    "\n",
    "**Complete the missing steps**:\n",
    "- Choose an action using an $\\epsilon$-greedy policy (use the function ``get_egreedy_action(.)`` we tested in section 2)\n",
    "- Update our q-function using a greedy (max) policy (use ``q_function[cur_state][action]`` to index our q-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(grid_env, alpha=0.1, min_epsilon=0.01, max_epsilon=1.0, \n",
    "               epsilon_decay = 80.0, n_episodes=500):\n",
    "    \"\"\"\n",
    "    This function computes an approximately optimal policy using q-learning\n",
    "    \n",
    "    :param grid_env (GridEnv): MDP environment\n",
    "    :param alpha (float): step-size\n",
    "    :param epsilon (float): value used during e-greedy action selection\n",
    "    :return: (dict) State-values for all non-terminal states\n",
    "    \"\"\"\n",
    "        \n",
    "    # This lambda function iteratively decreases epsilon\n",
    "    epsilon_by_episode = lambda ep_idx: min_epsilon + (max_epsilon - min_epsilon) * math.exp (-1 * ep_idx/epsilon_decay)\n",
    "    \n",
    "    # Obtain list of all states in environment\n",
    "    states = grid_env.get_states()\n",
    "    actions = grid_env.get_actions()\n",
    "    q_function = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for s in states:\n",
    "        for a in actions:\n",
    "            q_function[s][a] = 0\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        cur_state = grid_env.reset()\n",
    "        done = False\n",
    "        epsilon = epsilon_by_episode(i_episode)\n",
    "        \n",
    "        while not done:\n",
    "            # TODO 1: Complete off-policy action selection (e-greedy)\n",
    "            \n",
    "            next_state, reward, done,_ = grid_env.step(action)\n",
    "            q_next_state = list(q_function[next_state].values())\n",
    "            \n",
    "            # TODO 2: Complete update of q-function\n",
    "            \n",
    "            cur_state=next_state\n",
    "    \n",
    "    return decode_policy(grid_env, q_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test our implementation and compare our free-model policy with the one we obtained in the last lecture using value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "q_learning_policy = q_learning(grid_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=2, figure=fig)\n",
    "f_ax1 = fig.add_subplot(spec[0, 0])\n",
    "f_ax2 = fig.add_subplot(spec[0, 1])\n",
    "f_ax3 = fig.add_subplot(spec[1, 0])\n",
    "f_ax4 = fig.add_subplot(spec[1, 1])\n",
    "\n",
    "#Plot policy obtained using value-iteration value function\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=0)\n",
    "value_function, optimal_policy = value_iteration(grid_world)\n",
    "\n",
    "plot_policy(grid_world, optimal_policy, f_ax1)\n",
    "f_ax1.set_title(\"Policy - Value Iteration\")\n",
    "\n",
    "plot_policy(grid_world, q_learning_policy, f_ax2)\n",
    "f_ax2.set_title(\"Policy - Q-learning\")\n",
    "\n",
    "# Compute value function for q_learning policy\n",
    "q_policy_state_values = policy_evaluation(grid_world, encode_policy(grid_world, q_learning_policy))\n",
    "\n",
    "plot_value_function(grid_world, value_function, f_ax3)\n",
    "f_ax3.set_title(\"Value Function - Value Iteration\")\n",
    "\n",
    "plot_value_function(grid_world, q_policy_state_values, f_ax4)\n",
    "f_ax4.set_title(\"Value Function - Q-learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flux Question**: What is the main difference between the two policies? Is the q-learning policy optimal?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentRobotics",
   "language": "python",
   "name": "intelligentrobotics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
