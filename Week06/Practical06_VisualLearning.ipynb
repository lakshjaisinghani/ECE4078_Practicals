{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Practical 06: Visual Learning</h1> \n",
    "<h2 align=\"center\">Image classification with multi-layer perceptron</h2> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#0000FF\";> <b>Lets first make sure that all the required dependencies are installed</b></p> \n",
    "\n",
    "### With Conda (in your local machine)\n",
    "\n",
    "1. Open Anaconda prompt\n",
    "2. If using a GPU, type ``conda install pytorch torchvision cudatoolkit=10.2 -c pytorch``\n",
    "3. If using a CPU, type ``conda install pytorch torchvision cpuonly -c pytorch``\n",
    "\n",
    "**Side note**: I found that the installation with conda took a long time. ``pip`` might speed the process.\n",
    "\n",
    "### With pip (in your local machine)\n",
    "1. Open Anaconda prompt\n",
    "2. If using a GPU, type ``pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html``\n",
    "3. If using a CPU, type ``pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html``\n",
    "\n",
    "### In AWS\n",
    "1. Kernels with ``torchvision`` and ``pytorch`` installed are ready to be used\n",
    "2. Go to ``Kernel -> Change kernel`` and select ``conda_pytorch_latest_p36``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import time for timekeeping\n",
    "import time\n",
    "\n",
    "# Pytorch (Our Deep Learning Framework)\n",
    "import torch\n",
    "\n",
    "# Torch Data Loader (this will be helful to load image)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# datasets have mnist if using coustom images import io from skimage\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "# stores different optimizors like SGD\n",
    "import torch.optim as optim\n",
    "\n",
    "# Some torch functions that are used multiple times\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# Change this flag if using a GPU\n",
    "FLAG_GPU = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Define a Multi Layer Perceptron \"__ init __ \" function\n",
    "* Any network has an * __ init __ * function that initializes all the layers of the NN that require learnable parameters.\n",
    "* A MLP is a stack of fully connected layers. In this example we use three fully connected layers named :''fc0'', ''fc1'' and ''fc2''.\n",
    "* Note that each fully connected layer has a number of input neurons that connect to a number of output neurons. \n",
    "* These input and output dimensions are specified in the fc layer initialization.\n",
    "* If a fully connected layer connects to another, its output size = input size of the fully connected layer that follows.\n",
    "* The number of paramenters in any fully connected layer is #Input x #Output (and 1 bias per output).\n",
    "\n",
    "## How do we write a forward function?\n",
    "* torch.flatten(x, start_dim = dim) converts an image-like entity to a vector.\n",
    "* Remeber that you need activations after every fully connected layer. In this case we use ReLu. \n",
    "* Notice the log_sofmax layer at the end. This is a softmax activation function followed by log function as the name suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        \n",
    "        # First fully connected layer's input image is 28x28 = 784 dim.\n",
    "        self.fc0 = nn.Linear(784, 256) # nparam = 784*256 = 38400\n",
    "        \n",
    "        # Two more fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flattens the image-like structure into vectors\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "\n",
    "        # fully connected layers with activations\n",
    "        x = self.fc0(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        # Outputs are log(p) so softmax followed by log.\n",
    "        #return(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing an instance of the defined network here.\n",
    "* Note that putting a network to GPU is as simple as writing .cuda() at the end of the instance.\n",
    "* Same is true for a variable. In this  notebook the code inside the command \"if FLAG_GPU\" shows all the modifications you need to run your code on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLPNet()\n",
    "if FLAG_GPU:\n",
    "    net.cuda()\n",
    "    print(net)\n",
    "else:\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders and Transforms.\n",
    "* dataset.MNIST in pytorch implements the functionality to download and process MNIST data.\n",
    "* dataloader function usually allows for loading parts of training and test data in minibatches.\n",
    "* It can use some simple transformations implemented in class transforms that assists training. For example normalizing, resizing or cropping images.\n",
    "* The functionality of the dataset, transform and dataloader classes are usually added to suit new data and training proceedure related to the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "# Training dataset and training loader.\n",
    "trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Test dataset and loader.\n",
    "testset = datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we see sample usage of loading some MNIST training data.\n",
    "* What does our training minibatch look like?\n",
    "* At times simple visualization and print statements allow for understanding/debugging effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, l):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    print('Labels were:')\n",
    "    print(l.reshape(-1,8).numpy())\n",
    "\n",
    "# Load sample data\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print('shape of images', images.shape)\n",
    "\n",
    "# display batch\n",
    "imshow(utils.make_grid(images),labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function for learning.\n",
    "* NLLLoss: The abbrivation NLL stands for Negetive log likelihood. It is however a bit of misnomer as the log is not included in the loss itself but was part of the network defination above. \n",
    "* NOTE: When you want to get the probability/likelihood of an image being of a perticular class you need to remove the log from the forward function and use simple softmax activation at test time. Alternatively simply use ''exp'' function from torch to invert log and leave the forward function as it is. \n",
    "\n",
    "## Optimizer\n",
    "* pytorch has various optimization rutines (beyond SGD) pre-implemented.\n",
    "* class optim will take care of backpropogation with these different optimizations for learning as long as the network definition with appropriate forward function is written correctly.\n",
    "* Here we just use SGD with learning rate 0.001 and momentum 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "if FLAG_GPU:\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell of the notebook is training the network.\n",
    "\n",
    "* The first for loop goes through the entire data 5 times (We run 5 epochs for our training).\n",
    "* The simple steps for training a NN with pytorch are:\n",
    "    * Load data in minibatches.\n",
    "    * Set gradients for all the network parameters to zero (dont forget this)\n",
    "    * Pass data to the NN using a net.forward() to compute layer by layer output.\n",
    "        * Intermediate outputs can be returned as extra variables in forward function.\n",
    "    * Compute the loss from the output (remember it is defined above).\n",
    "    * Use loss.backword() to compute all the gradients by appropriately applying chain rule! \n",
    "        * It actually knows how to differentiate things!!!\n",
    "    * Use optimizer.step() to update the weights.\n",
    "    \n",
    "## At the end of every epoch usually we check if NN generalizes.\n",
    "* Generalization is critical in learning.\n",
    "* We evaluate the performance of our NN on new data, for which the NN loss was not minimized.\n",
    "* The torch.no_grad() command forces the following code to not keep track of the gradients as for testing we don't need them.\n",
    "* As no gradients are maintained, the code runs faster!\n",
    "* It is very good practice to make use of no_grad function to ensure that we dont accidently minimize loss on the data we are testing the performance on.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Simply for time keeping\n",
    "    start_time = time.time()\n",
    "    # Loop over all training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    " \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        if FLAG_GPU:\n",
    "            outputs = net(inputs.cuda())\n",
    "            loss = criterion(outputs, labels.cuda())\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Compute Gradients\n",
    "        loss.backward()\n",
    "        # BackProp\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        # endif\n",
    "    # end for over minibatches epoch finishes\n",
    "    end_time = time.time()\n",
    "\n",
    "    # test the network every epoch on test example\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Test after the epoch finishes (no gradient computation needed)\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            # load images and labels\n",
    "            images, labels = data\n",
    "\n",
    "            if FLAG_GPU:\n",
    "                outputs = net(images.cuda())\n",
    "                # note here we take the max of all probability\n",
    "                _, predicted = torch.max(outputs.cpu(), 1)\n",
    "            else:\n",
    "                outputs = net(images)\n",
    "                # note here we take the max of all probability\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "      #end for\n",
    "    #end with\n",
    "    print('Epoch', epoch+1, 'took', end_time-start_time, 'seconds')\n",
    "    print('Accuracy of the network after', epoch+1, 'epochs is' , 100*correct/total)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Passive Inference)",
   "language": "python",
   "name": "passiveinference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
