{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Practical 07: Robot Control</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.linalg as linalg\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('Support'))\n",
    "\n",
    "from Robot import *\n",
    "from Helper import *\n",
    "from Renderer import Renderer as PenguinPiRenderer\n",
    "from Renderer1D import RobotRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See docstring for functions in support scripts**: If you want to know what a function does, just click somewhere within the parentheses that enclose the arguments and hit SHIFT+TAB. If there's a + button at the top of the popup tooltip, this means the documentation spans a few lines, click it to show the full docstring, then scroll up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PID Control - 1D Linear System\n",
    "\n",
    "Recall our 1D robot \n",
    "\n",
    "<img src=\"Support/images/1DRobot.png\" width=\"400\" height=\"400\" align=\"center\">\n",
    "\n",
    "We want to implement a controller that generates the signal $\\mathbf{u}_t$ required to bring the robot to a desired state $\\mathbf{x}_d$\n",
    "\n",
    "We first define our robot in State-Space Form:\n",
    "\n",
    "<center> $\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t$ </center>\n",
    "<center> $\\mathbf{y}_{t} = C\\mathbf{x}_t$, </center>\n",
    "\n",
    "where $A$ corresponds to the state matrix, $B$ defines the input or control matrix and $C$ corresponds to the output matrix. Our robot will be represented by the Python class ``Robot1D``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot1D(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A simple implementation of a 1D robot in state-space form\n",
    "    The constant MAX_CONTROL bounds the magnitude of the control signal that can be applied to the robot \n",
    "    \"\"\"\n",
    "\n",
    "    MAX_CONTROL = 5\n",
    "    \n",
    "    def __init__(self, A=np.eye(2), B=np.array([[0],[0]]), C=np.array([0, 0]), initial_state=np.array([[10],[0]])):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a new model. \n",
    "        :param A: nxn state matrix, where n = state dimensionality\n",
    "        :param B: nx1 input matrix, where n = state dimensionality\n",
    "        :param C: 1xn input matrix, where n = state dimensionality\n",
    "        :param initial_state: 2x1 vector with the initial state of our system\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.state = initial_state\n",
    "                \n",
    "    def drive(self, control_u=10):\n",
    "        \"\"\"\n",
    "        Update the system's state given a new control input\n",
    "        :param control_u: Control input\n",
    "        \"\"\"\n",
    "        # Make sure control signal is within -MAX_CONTROL < control_u < MAX_CONTROL\n",
    "        clip_control = np.clip(control_u, -self.MAX_CONTROL, self.MAX_CONTROL)\n",
    "        state_t1 = self.A @ self.state + self.B * clip_control\n",
    "        self.state = state_t1\n",
    "            \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the system's current state\n",
    "        \"\"\"\n",
    "        return self.state\n",
    "    \n",
    "    def get_output(self):\n",
    "        \"\"\"\n",
    "        Get the system's ouput (position of the robot along the 1D line)\n",
    "        \"\"\"\n",
    "        return self.C @ self.state\n",
    "                \n",
    "    def get_error(self, desired_x):\n",
    "        \"\"\"\n",
    "        This method measures the error (scalar) between the current robot's state and the desired state\n",
    "        :param desired_x: Desired state (i.e., position) on the 1D line\n",
    "        \"\"\"\n",
    "        return (desired_x - self.state)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our PID Controller\n",
    "\n",
    "Let us now define our PID controller. Recall the formulation of a PID controller in the discrete domain:\n",
    "\n",
    "<center>$\\mathbf{u}_t = K_p \\mathbf{e}_t + K_i \\sum_{k = 0}^t \\mathbf{e}_k \\Delta t + K_d \\frac{\\mathbf{e}_t - \\mathbf{e}_{t-1}}{\\Delta t}$, where</center>\n",
    "\n",
    "- $K_p, K_i, K_d$ are the proportional, integral and derivative gains\n",
    "- $\\mathbf{e}_t$ is the error at time $t$. In our case $\\mathbf{e}_t = \\mathbf{x}_d - \\mathbf{x}_t$\n",
    "- $\\Delta t$ is the rate at which the PID controller is updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PID(object):\n",
    "    \n",
    "    \"\"\"A simple PID controller.\"\"\"\n",
    "\n",
    "    def __init__(self, system=None, desired_state=None, K_p=0, \n",
    "                 K_i=0, K_d=0, update_rate=0.05):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a new PID controller.\n",
    "        :param system: The robot system to controlled\n",
    "        :param desired_state: The goal state that the PID will try to achieve for the robotic system\n",
    "        :param K_p: The value for the proportional gain K_p\n",
    "        :param K_i: The value for the integral gain K_i\n",
    "        :param K_d: The value for the derivative gain K_d\n",
    "        :param update_rate: Rate at which the PID controller is updated. \n",
    "        \"\"\"\n",
    "        self.system = system\n",
    "        self.set_point = desired_state\n",
    "        self.K_p = K_p\n",
    "        self.K_i = K_i\n",
    "        self.K_d = K_d\n",
    "        self.update_rate = update_rate\n",
    "        \n",
    "        # Last observed error between desired state (self.set_point) and the current state of the system\n",
    "        self.last_error = self.system.get_error(self.set_point)\n",
    "        \n",
    "        # Accumulator for integral term\n",
    "        self.integral = 0\n",
    "        \n",
    "        # Last time the controller was called\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "        # Last control command provided to the system\n",
    "        self.last_control = 0\n",
    "        \n",
    "    def compute_control(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method computes the next control signal u_t so as to reduce the error\n",
    "        between the robot's current state and the desired state (self.set_point)\n",
    "        \"\"\"\n",
    "        \n",
    "        current_time = time.time()\n",
    "        \n",
    "        # We compute the time elapsed (i.e., delta_time) since the last call to the controller\n",
    "        # If current_time - self.last_time is zero, we set elapsed_time = 1e-16 to avoid division by zero\n",
    "        delta_time = current_time - self.last_time if current_time - self.last_time else 1e-16\n",
    "        \n",
    "        # We return last control signal if time elapsed since last call is less than self.update_rate\n",
    "        if delta_time < self.update_rate:\n",
    "            return self.last_control\n",
    "        \n",
    "        # Compute error\n",
    "        error_t = self.system.get_error(self.set_point)\n",
    "        \n",
    "        # Compute integral and derivative terms\n",
    "        derivative = (error_t - self.last_error) / delta_time\n",
    "        self.integral += error_t * delta_time\n",
    "        \n",
    "        # Compute new control\n",
    "        new_control = self.K_p * error_t + self.K_i * self.integral + self.K_d * derivative\n",
    "        \n",
    "        # Update self.last_error, self.last_time and self.last_control signal with new values\n",
    "        self.last_error = error_t\n",
    "        self.last_time = current_time\n",
    "        self.last_control = new_control\n",
    "                \n",
    "        return new_control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Controller\n",
    "\n",
    "Let us now test our controller. We consider the following values in our current test case:\n",
    "- The robot initial state is $\\mathbf{x}_0 = (10, 0)$. The desired states is $\\mathbf{x}_d = (50, 0)$\n",
    "- The PID gains are set to $K_p=2, K_i=0, K_d=0.001$\n",
    "- Our control loop lasts one second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state, input and output matrices\n",
    "A = np.array([[1, 0], [1, 0]])\n",
    "B = np.array([[0.5],[0]])\n",
    "C = np.array([1, 0])\n",
    "\n",
    "# Create a robot and define desired state\n",
    "bot = Robot1D(A=A, B=B, C=C, initial_state=np.array([[10],[0]]))\n",
    "desired_x = np.array([[50],[0]])\n",
    "\n",
    "# Create a PID controller\n",
    "pid = PID(system=bot, desired_state=desired_x, K_p=2, K_i=0, K_d=0.001)\n",
    "\n",
    "# Variables needed for plotting\n",
    "u, sim_time, error, robot_state = [], [], [], []\n",
    "\n",
    "# Keep track of time within the control loop\n",
    "start_time = time.time()\n",
    "last_time = start_time\n",
    "\n",
    "# This is our control loop. Currently we call our controller for 1 second\n",
    "while time.time() - start_time < 1:\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Call controller to get new signal u_t\n",
    "    u_t = pid.compute_control()\n",
    "    # Apply control to robot\n",
    "    bot.drive(u_t)\n",
    "\n",
    "    # Save values for plotting\n",
    "    sim_time += [current_time - start_time]\n",
    "    error += [bot.get_error(desired_x)]\n",
    "    u += [u_t]\n",
    "    robot_state += [bot.get_output()]\n",
    "    \n",
    "    last_time = current_time\n",
    "    \n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Resulting Trajectories\n",
    "\n",
    "Let us now plot the changes observed in error (right-bottom), control signal (right-top), and robot's state (left) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define renderer and start animation\n",
    "rend = RobotRenderer.Instance()\n",
    "rend.initialize(robot_state, u, error, sim_time, desired_x, dt_render=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>FLUX Question (1pt): </font> \n",
    "What happens to the tracking error when we increase the proportional gain $K_p$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PID Control - Differential Drive Vehicle\n",
    "\n",
    "Recall the kinematic model of our PenguinPi robot:\n",
    "\n",
    "<img src=\"Support/images/PenguinPi_Model.png\" width=\"400\" height=\"400\" align=\"center\">\n",
    "\n",
    "In this model, the state is defined by a 3D vector ($x, y, \\theta)$ and the control input corresponds to the linear and angular velocities $(v, \\omega)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Move to Goal Controller\n",
    "\n",
    "Consider the problem of moving the PenguinPi to a goal point $(x_g, y_g)$. How can we solve this control problem?\n",
    "\n",
    "Let us define 2 proportional controllers:\n",
    "- One controller with gain $K_{pw}$ that will turn the robot toward the goal. This controller determines the angular velocity $\\omega_k$ according to\n",
    "\n",
    "<center> $\\omega_k = K_{pw} (\\theta_g - \\theta_r)$, </center> \n",
    "\n",
    "where $\\theta_g$ corresponds to the angle to the goal relative to the heading of the robot and $\\theta_r$ is the heading of the robot at time $k$.\n",
    "\n",
    "- One controller with gain $K_{pv}$ that will keep moving the robot forward until it reaches the goal. This controller determines the angular velocity $v_k$ according to\n",
    "\n",
    "<center> $v_k = K_{pv}\\sqrt{(y_g-y_k)^2+(x_g-x_k)^2}$ </center> \n",
    "\n",
    "We have combined these 2 controllers into a single class called ``MoveToGoalController``. Two helper functions ``get_distance_robot_to_goal(.)`` and ``get_angle_robot_to_goal(.)`` have been defined to compute $(\\theta_g - \\theta_r)$ and $\\sqrt{(y_g-y_k)^2+(x_g-x_k)^2}$ respectively.\n",
    "\n",
    "\n",
    "#### TODO: \n",
    "- Compute both $\\omega_k$ and $v_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveToGoalController(object):\n",
    "    \n",
    "    \"\"\"A simple move to goal proportional controller\"\"\"\n",
    "\n",
    "    def __init__(self, robot=None, K_pw=0, K_pv=0):\n",
    "        \"\"\"\n",
    "        Initialize a new move to goal proportional controller\n",
    "        :param robot: The robotic system to controlled\n",
    "        :param K_w: The proportional gain for the angular velocity\n",
    "        :param K_v: The proportional gain for the linear velocity\n",
    "        \"\"\"\n",
    "        self.robot = robot\n",
    "        self.K_pw = K_pw\n",
    "        self.K_pv = K_pv\n",
    "        \n",
    "        \n",
    "    def run(self, goal_position=np.zeros(2), threshold=1e-3, delta_time=0.01):\n",
    "        \"\"\"\n",
    "        Run control loop until the robot reaches the goal_position\n",
    "        :param goal_position: Desired goal position\n",
    "        :param threshold: Value used to determine whether robot has reached \n",
    "                          the goal location\n",
    "        \"\"\"\n",
    "        last_time = time.time()\n",
    "        \n",
    "        states = []\n",
    "        controls = []\n",
    "        initial_state = self.robot.get_state()\n",
    "        \n",
    "        distance_to_goal = get_distance_robot_to_goal(initial_state, goal_position)\n",
    "        desired_heading = get_angle_robot_to_goal(initial_state, goal_position)\n",
    "                \n",
    "        states.append(initial_state)\n",
    "        \n",
    "        while distance_to_goal > threshold:\n",
    "            \n",
    "            # ---------- TODO Compute control input----------------\n",
    "            v_k = 0\n",
    "            w_k = 0\n",
    "            # -----------------------------------------------------\n",
    "            \n",
    "            # Apply control to robot\n",
    "            self.robot.drive(v_k, w_k, delta_time)\n",
    "            new_state = self.robot.get_state()\n",
    "            \n",
    "            # Keep track of variables for plotting\n",
    "            controls.append([v_k, w_k])\n",
    "            states.append(new_state)\n",
    "                        \n",
    "            # Update distance and desired heading\n",
    "            distance_to_goal = get_distance_robot_to_goal(new_state, goal_position)\n",
    "            desired_heading = get_angle_robot_to_goal(new_state, goal_position)\n",
    "                       \n",
    "        return states, controls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Controller\n",
    "\n",
    "Let us now visualise how our PenguinPi robot moves toward a goal position using the control signals computed by our ``MoveToGoalController``\n",
    "\n",
    "We consider the following values in our current test case:\n",
    "- The robot initial state is $\\mathbf{x}_0 = (-1.5, -1.5, \\frac{\\pi}{3})$. The desired states is $\\mathbf{x}_d = (1.0, 1.0)$.\n",
    "- The proportional gains are set to $K_{pw}=5, K_{pv}=2$\n",
    "- Our control loop lasts until $\\sqrt{(y_g-y_k)^2+(x_g-x_k)^2} > 0.001$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define robot, desired goal and delta time\n",
    "bot = PenguinPi(init_state=np.array([-1.5, -1.5, np.pi/3]))\n",
    "desired_goal = np.array([1.0, 1.0, 0])\n",
    "delta_t = 0.01\n",
    "\n",
    "# Instantiate controller and compute sequence of control signals\n",
    "controller = MoveToGoalController(bot, K_pw=5, K_pv=2)\n",
    "robot_states, robot_controls = controller.run(goal_position=desired_goal, delta_time=delta_t)\n",
    "\n",
    "# Define variables for animation\n",
    "array_states = np.array(robot_states)\n",
    "array_controls = np.array(robot_controls)\n",
    "\n",
    "# Define renderer and start animation\n",
    "rend = PenguinPiRenderer.Instance()\n",
    "rend.initialize(array_states, array_controls, target_pose=desired_goal, dt_data=delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>FLUX Question (1pt): </font> \n",
    "Currently $K_{pw} > K_{pv}$, what happens if we set $K_{pw} = K_{pv} = 5$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LQR - Discrete Time Infinite Horizon\n",
    "\n",
    "Given a linear system with equations:\n",
    "\n",
    "<center> $\\mathbf{x}_{t+1} = A\\mathbf{x}_t + B\\mathbf{u}_t$ </center>\n",
    "<center> $\\mathbf{y}_{t} = C\\mathbf{x}_t$, </center>\n",
    "\n",
    "where $A$ corresponds to the state matrix, $B$ defines the input or control matrix and $C$ corresponds to the output matrix.\n",
    "\n",
    "We wan to find $\\mathbf{u}$ so as to minimize the cost function\n",
    "\n",
    "<center>$J = \\sum_{t=0}^{\\infty} ((\\mathbf{x}_t-\\mathbf{x}_g)^TQ(\\mathbf{x}_t-\\mathbf{x}_g) + \\mathbf{u}_t^TR\\mathbf{u})$, </center> \n",
    "\n",
    "where $Q$ defines the state cost and $R$ corresponds to the control cost.\n",
    "\n",
    "To do so, we want to implement a **LQR Infinite Horizon Controller** with gain  \n",
    "<center> $K = (R+B^TPB)^{-1}(B^TPA)$ </center>\n",
    "\n",
    "**TODO:**\n",
    "- Complete the definition of the controller's gain K. You can use the function ``linalg.inv(.)`` to compute the inverse of a matrix\n",
    "- Keep in mind that $P$ is defined by the class attribute ``self.P``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteInfiniteLQR(object):\n",
    "    \n",
    "    \"\"\"A simple LQR Infinite Horizon controller.\"\"\"\n",
    "\n",
    "    def __init__(self, Q=np.eye(2), R=0.3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a new LQR controller.\n",
    "        :param Q: State cost\n",
    "        :param R: Control cost\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        \n",
    "        # Controller gain to be defined once LQR function is solved\n",
    "        self.K = np.zeros(2)\n",
    "        self.P = np.eye(2)\n",
    "                \n",
    "    def solve(self, system):\n",
    "        \"\"\"\n",
    "        Compute controller gain for a given system\n",
    "        :param system: Linear system for which a control law needs to be\n",
    "                       computed\n",
    "        \"\"\"\n",
    "        A = system.A\n",
    "        B = system.B\n",
    "        \n",
    "        # Solve Riccati Equation\n",
    "        self.P = linalg.solve_discrete_are(A, B, self.Q, self.R)\n",
    "                \n",
    "        # ----------------------- TODO: Compute the controller's gain ------------\n",
    "        self.K = 0\n",
    "        \n",
    "    def get_gain(self):\n",
    "        \"\"\"\n",
    "        Get access to controller gain\n",
    "        \"\"\"\n",
    "        return self.K\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Controller\n",
    "\n",
    "Let us now take a look at the state and control trajectories that can obtained using our LQR implementation. Keep in mind that the optimal controller is defined as\n",
    "\n",
    "<center>$\\mathbf{u_t} = -K(\\mathbf{x_t} - \\mathbf{x_g})$, </center>\n",
    "\n",
    "where $\\mathbf{x_g}$ corresponds to the desired state.\n",
    "\n",
    "We consider the same test case used with our PID controller:\n",
    "- The robot initial state is $\\mathbf{x}_0 = (10, 0)$. The desired states is $\\mathbf{x}_d = (50, 0)$\n",
    "- Our control loop lasts one second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-61990ea63b63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Determine and apply control\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mu_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlqr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdesired_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# Create an instance of our 1D robot system\n",
    "A = np.array([[1, 0], [1, 0]])\n",
    "B = np.array([[0.5],[0]])\n",
    "C = np.array([1, 0])\n",
    "\n",
    "bot = Robot1D(A=A, B=B, C=C, initial_state=np.array([[10], [0]]))\n",
    "\n",
    "# Define the state and control costs\n",
    "Q = np.array([[1,0],[0,0]])\n",
    "R = 0.3\n",
    "\n",
    "# Create an instace of our controller and solve the LQR problem\n",
    "lqr = DiscreteInfiniteLQR(Q=Q, R=R)\n",
    "lqr.solve(bot)\n",
    "\n",
    "# Define desired state\n",
    "desired_x = np.array([[50],[0]])\n",
    "\n",
    "# Keep track of values for plotting\n",
    "u_list, sim_time, error, y_list = [], [], [], []\n",
    "\n",
    "# Keep track of time within the control loop\n",
    "start_time = time.time()\n",
    "last_time = start_time\n",
    "\n",
    "# This is our control loop. Currently we call our controller for 1 second\n",
    "while time.time() - start_time < 1:\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Get current state of the system\n",
    "    x_t = bot.get_state()\n",
    "    # Get output \n",
    "    y_t = bot.get_output()\n",
    "    \n",
    "    # Determine and apply control\n",
    "    u_t = (-lqr.K @ (x_t - desired_x))[0]\n",
    "    bot.drive(u_t)\n",
    "    \n",
    "    # Keep track of variable for plotting\n",
    "    sim_time += [current_time - start_time]\n",
    "    u_list += [u_t]\n",
    "    y_list += [y_t]\n",
    "    error += [bot.get_error(desired_x)]\n",
    "        \n",
    "    last_time = current_time\n",
    "    \n",
    "    time.sleep(0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Resulting Trajectories\n",
    "\n",
    "Let us now plot the changes observed in error (right-bottom), control signal (right-top), and robot's state (left) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define renderer and start animation\n",
    "rend = RobotRenderer.Instance()\n",
    "rend.initialize(y_list, u_list, error, sim_time, desired_x, dt_render=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='blue'>FLUX Questions (1pt each): </font> \n",
    "1. What happens if we make the control cost more important than the state cost (i.e., modify R and Q variables)?\n",
    "2. Now that you have seen the trajectories that can be obtained using a PID or LQR controller, why do you think the PID controller fails to reach the desired state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (3 pts): Implement a Pose Controller for the PenguinPi Robot\n",
    "\n",
    "In Part 2 of this notebook, we defined a proportional controller that brings our PenguinPi Robot to a desired location in space. However, we did not control the robot's orientation.\n",
    "\n",
    "We now want to define a controller that brings our robot to a goal pose that includes a desired orientation $(x_g, y_g, \\theta_g)$. This controller, represented by the class ``MoveToGoalPoseController``, combines three proportional controllers with gains $K_\\rho, K_\\alpha, K_\\beta$. \n",
    "\n",
    "The control law of this new controller is defined as follows\n",
    "\n",
    "<center>\n",
    "$\\begin{equation}\n",
    "\\begin{split}\n",
    "v_t &= K_\\rho \\rho\\\\\n",
    "\\omega_t &= K_\\alpha \\alpha + K_\\beta \\beta, \\\\\n",
    "\\end{split}\n",
    "\\end{equation}$\n",
    "</center>\n",
    "\n",
    "where \n",
    "- $\\rho = \\sqrt{(y_g-y_t)^2+(x_g-x_t)^2}$ corresponds to the distance between the robot and the goal position;\n",
    "- $\\alpha = \\text{tan}^{-1} \\frac{(y_g-y_t)}{(x_g-x_t)} - \\theta_t$ defines the angle to the goal relative to the heading of the robot; and\n",
    "- $\\beta = \\theta_g -\\theta_t - \\alpha$ corresponds the angle between the robot's position and the goal position plus the goal angle.\n",
    "\n",
    "**Things to keep in mind**\n",
    "- Compute $\\rho$, $\\alpha$, and $\\beta$ with the help of the functions defined in ``Helper.py`` (see scripts in *Support* folder)\n",
    "- Limit $\\beta$ to the range $[-\\pi, \\pi]$ to prevent unstable behavior. You can use the helper function ``clamp_angle(.)`` to do so.\n",
    "\n",
    "**Grading**\n",
    "- Make sure that the ``run(.)`` function returns two arrays. The first array of dimensions 3xN corresponds to states. The second array of dimensions 2XN corresponds to the inputs (or controls) computed by your controller\n",
    "- <font color='red'>**Remove all print statements before submitting your solution**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoveToPoseController(object):\n",
    "    \n",
    "    \"\"\"A simple move to pose proportional controller\"\"\"\n",
    "\n",
    "    def __init__(self, robot=None, K_rho=0, K_alpha=0, K_beta=0):\n",
    "        \"\"\"\n",
    "        Initialize a new move to pose proportional controller\n",
    "        :param robot: The robotic system to controlled\n",
    "        :param K_rho: The proportional gain for the linear velocity \n",
    "        :param K_alpha: First proportional gain for the angular velocity\n",
    "        :param K_beta: Second proportional gain for the angular velocity\n",
    "        \"\"\"\n",
    "        self.robot = robot\n",
    "        self.K_rho = K_rho\n",
    "        self.K_alpha = K_alpha\n",
    "        self.K_beta = K_beta\n",
    "        \n",
    "        \n",
    "    def run(self, goal_position=np.zeros(3), threshold=1e-3, delta_time=0.01):\n",
    "        \"\"\"\n",
    "        Run control loop until the robot reaches the goal_position\n",
    "        :param goal_position: Desired goal position\n",
    "        :param threshold: Value used to determine whether robot has reached \n",
    "                          the goal location\n",
    "        :returns: Tuple of arrays. A 3DxN array for the system states and a 2DxN array for control inputs\n",
    "        \"\"\"\n",
    "        last_time = time.time()\n",
    "        \n",
    "        states = []\n",
    "        controls = []\n",
    "        initial_state = self.robot.get_state()\n",
    "        states.append(initial_state)\n",
    "        \n",
    "        # ---------- TODO 1: Compute rho, alpha and beta\n",
    "                        \n",
    "        # ------------ TODO 2: Implement the control loop\n",
    "        while rho > threshold:\n",
    "            \n",
    "            # 1.Compute the new control input\n",
    "            \n",
    "            # 2. Apply control to robot and get new state\n",
    "            \n",
    "            # 3. Keep track of control and states for plotting. Use the states and controls list for this\n",
    "                        \n",
    "            # 3. Update rho, alpha and beta\n",
    "                       \n",
    "        return np.array(states), np.array(controls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your Solution\n",
    "\n",
    "**Visually**: The position and orientation of your robot should match the target pose currently displayed as a fixed 2D coordinate frame at (1.0, 1.0)\n",
    "\n",
    "**Numerically**: Compare your states and controls to the values in the test file ``Support/data/MoveToPose_TestCase.pk``:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "with open('Support/data/MoveToPose_TestCase.pk', 'wb') as read_from:\n",
    "    test_values = pickle.load(read_from)\n",
    "    \n",
    "test_controls = test_values['control']\n",
    "test_states = test_values['states'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define robot, desired goal and delta time\n",
    "bot = PenguinPi(init_state=np.array([-1.5, -1.5, 0]))\n",
    "desired_goal = np.array([1.0, 1.0, np.pi/2])\n",
    "delta_t = 0.01\n",
    "\n",
    "# Instantiate controller and compute sequence of control signal\n",
    "controller = MoveToPoseController(bot,  K_rho=3, K_alpha=7, K_beta=-1)\n",
    "robot_states, robot_controls = controller.run(goal_position=desired_goal, delta_time=delta_t)\n",
    "\n",
    "# Define renderer and start animation\n",
    "rend = PenguinPiRenderer.Instance()\n",
    "rend.initialize(robot_states, robot_controls, target_pose=desired_goal, dt_data=delta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (3 pts): Implement a Discrete Time Finite Horizon LQR Controller\n",
    "\n",
    "The solution to the discrete-time finite horizon LQR problem is given by the following algorithm:\n",
    "\n",
    "1. Set $P_N$ = Q\n",
    "2. For $t = N,\\dots,1$\n",
    "\n",
    "<center>$P_{t-1} = Q + A^TP_tA - A^TP_tB(R+B^TP_tB)^{-1}B^TP_tA$</center>\n",
    "\n",
    "3. For $t = 0,\\dots,N-1$, define\n",
    "<center>$K_{t} = -(R+B^TP_{t+1}B)^{-1}B^TP_{t+1}A$</center>\n",
    "\n",
    "4. For $t = 0,\\dots,N-1$, optimal $\\mathbf{u}_t$ is given by <center>$\\mathbf{u}_t = -K_t(\\mathbf{x}_t - \\mathbf{x}_g$)</center>\n",
    "\n",
    "You are tasked with defining the corresponding ``DiscreteFiniteLQR`` Python class. A skeleton class is provided below. \n",
    "\n",
    "**Please keep in mind**\n",
    "- ``self.P`` = $[P_1, P_2,\\dots,P_N]$. Each time you compute a new P matrix, add it to the self.P attribute (defined as a list) using the method ``self.P.append(new_P)``. All P matrices are needed for the computation of the gains.\n",
    "- ``self.K`` = $[K_0, K_1,\\dots,K_{N-1}]$. Each time you compute a new gain K, add it to the self.K attribute (defined as a list) using the method ``self.K.append(new_K)``. All K matrices are needed for the computation of the control inputs.\n",
    "\n",
    "**Grading**\n",
    "- You will be graded based on the output of the ``run_control_loop_method(.)``. Please make sure that your solution returns the expected variables with the correct type.\n",
    "- <font color='red'>**Remove all print statements before submitting your solution**</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteFiniteLQR(object):\n",
    "    \n",
    "    \"\"\"A simple LQR controller.\"\"\"\n",
    "\n",
    "    def __init__(self, Q=np.eye(2), R=0.3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a new LQR controller.\n",
    "        :param Q: State cost\n",
    "        :param R: Control cost\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        \n",
    "        # List of solutions to the Riccati Equation for each iteration\n",
    "        self.P = []\n",
    "        # List of controller gains for each iteration\n",
    "        self.K = []\n",
    "        \n",
    "        \n",
    "    def solve(self, system, horizon=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute controller gain for a given system\n",
    "        :param system: Linear system for which a control law needs to be\n",
    "                       computed\n",
    "        :param horizon: Planning horizon\n",
    "        \"\"\"\n",
    "        \n",
    "        A = system.A\n",
    "        B = system.B\n",
    "        \n",
    "        # Set P_N = Q\n",
    "        self.P.append(self.Q)\n",
    "        \n",
    "        # ---------------- TODO 1: Compute all P matrices ---------------\n",
    "        # Do not forget to add each new P matrix to the class attribute self.P\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "        for i in range(horizon, 0, -1):\n",
    "            pass\n",
    "        \n",
    "        # ---------------- TODO 2: Compute all gains K  ---------------\n",
    "        # Do not forget to add new K gain to the class attribute self.K\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "        for i in range(horizon):\n",
    "            pass\n",
    "            \n",
    "                \n",
    "    def run_control_loop(self, system, desired_state, horizon):\n",
    "        \"\"\"\n",
    "        Compute control inputs required for the system to reach the desired state\n",
    "        :param system: Linear system for which a control inputs need to be\n",
    "                       computed\n",
    "        :param desired_state: Target state for the system\n",
    "        :param horizon: Planning horizon\n",
    "        :returns: Tuple of arrays, 1Dxhorizon array for the system outputs (i.e., position) and 1Dxhorizon array for control inputs\n",
    "        \"\"\"\n",
    "         \n",
    "        u_controls, y_outputs = [], []\n",
    "\n",
    "        for i in range(horizon):\n",
    "            \n",
    "            #--------------- TODO 3: Compute control inputs ------------------\n",
    "            \n",
    "            # 1. Get current state\n",
    "            # 2. Get system's output        \n",
    "            # 3. Compute control u_t and apply it to the robot\n",
    "            # 4. Add current state and control to return types\n",
    "        \n",
    "        return np.array(y_outputs), np.array(u_controls)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Solution\n",
    "\n",
    "Compute $\\mathbf{u}_t$ and $\\mathbf{y}_t$ using the gains obtained from your LQR controller. Your output will be compared to the values in the test file ``test_case.pk``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our system\n",
    "A=np.array([[1, 1], [1, 0]]) \n",
    "B=np.array([[0],[1]])\n",
    "C=np.array([1, 0])\n",
    "initial_state=np.array([[1], [0]])\n",
    "\n",
    "my_system = Robot1D(A=A, B=B, C=C, initial_state=initial_state)\n",
    "\n",
    "# Load test case and define variables\n",
    "with open('Support/data/test_case.pk', 'rb') as read_from:\n",
    "    test = pickle.load(read_from)\n",
    "\n",
    "# Define the state and control costs as well as the planning horizon\n",
    "Q = test['Q']\n",
    "R = test['R']\n",
    "horizon = test['horizon']\n",
    "\n",
    "# Create an instace of our controller and solve the LQR problem\n",
    "lqr = DiscreteFiniteLQR(Q=Q, R=R)\n",
    "lqr.solve(my_system, horizon)\n",
    "\n",
    "# Define desired state\n",
    "desired_x = np.array([[0], [0]])\n",
    "\n",
    "# Apply control loop\n",
    "y_array, u_array = lqr.run_control_loop(my_system, desired_x, horizon)\n",
    "\n",
    "# Plot and compare results\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "ax1.plot(np.arange(horizon), u_array, 'r', label='Your Solution')\n",
    "ax1.plot(np.arange(horizon), test['control'], 'g-.', label='Expected', lw=2)\n",
    "ax1.set_ylabel(r'$\\mathbf{u}_t$')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.plot(np.arange(horizon), y_array, 'b', label='Your Solution')\n",
    "ax2.plot(np.arange(horizon), test['output'], 'g-.', label='Expected', lw=2)\n",
    "ax2.set_ylabel(r'$\\mathbf{y}_t$')\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.legend(loc='best')\n",
    "\n",
    "ax1.set_title('LQR Finite Horizon Control Results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntelligentRobotics",
   "language": "python",
   "name": "intelligentrobotics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
